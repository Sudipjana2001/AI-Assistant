# Agent Architecture Documentation - Easy Guide

> **What is this?** This guide explains how our AI agents work in simple terms with lots of pictures and diagrams.

---

## ğŸ“– Table of Contents
1. [The Big Picture](#the-big-picture)
2. [How Does a Request Flow?](#how-does-a-request-flow)
3. [The Three Main Parts](#the-three-main-parts)
4. [Step-by-Step: What Happens When You Ask a Question?](#step-by-step-what-happens-when-you-ask-a-question)
5. [How to Create Your Own Agent](#how-to-create-your-own-agent)

---

## ï¿½ The Big Picture

**Think of it like a restaurant:**
- **You** (the customer) = User asking questions
- **Waiter** = Agent Orchestrator (decides which chef to send your order to)
- **Chefs** = Different Agents (Analyst, Researcher, Python Coder, etc.)
- **Recipe Book** = Azure AI (the smart brain that knows everything)
- **Pantry** = Data Storage (where files and information are kept)

```mermaid
graph LR
    User([ğŸ‘¤ You]) -->|Ask Question| Waiter[ğŸ½ï¸ Orchestrator]
    Waiter -->|Routes to| Chef1[ğŸ‘¨â€ğŸ³ Analyst Agent]
    Waiter -->|Routes to| Chef2[ğŸ‘¨â€ğŸ³ Researcher Agent]
    Waiter -->|Routes to| Chef3[ğŸ‘¨â€ğŸ³ Python Agent]
    
    Chef1 -->|Uses| RecipeBook[ğŸ“š Azure AI]
    Chef2 -->|Uses| RecipeBook
    Chef3 -->|Uses| RecipeBook
    
    Chef1 -->|Checks| Pantry[ğŸ—„ï¸ Data Storage]
    Chef2 -->|Checks| Pantry
    Chef3 -->|Checks| Pantry
    
    Chef1 -->|Returns Answer| User
    Chef2 -->|Returns Answer| User
    Chef3 -->|Returns Answer| User
```

---

## ğŸ”„ How Does a Request Flow?

When you ask a question, here's the journey it takes:

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ You
    participant Orch as ğŸ½ï¸ Orchestrator
    participant Agent as ğŸ‘¨â€ğŸ³ Agent
    participant Data as ğŸ—„ï¸ Data Layer
    participant AI as ğŸ“š Azure AI
    
    User->>Orch: "Analyze my sales data"
    Orch->>Agent: Route to Analyst Agent
    
    Note over Agent: Step 1: Get Context
    Agent->>Data: Search for "sales data"
    Data-->>Agent: Returns metadata only<br/>(filename, columns, etc.)
    
    Note over Agent: Step 2: Build Prompt
    Agent->>Agent: Combine question + metadata
    
    Note over Agent: Step 3: Ask AI
    Agent->>AI: Send prompt to Azure OpenAI
    AI-->>Agent: Returns intelligent response
    
    Agent->>User: Final Answer with insights
```

---

## ğŸ§© The Three Main Parts

### Part 1: BaseAgent (The Template)

**What is it?** Think of `BaseAgent` as a cookie cutter. Every agent is made from this same shape.

**What does it do?**
1. ğŸ” **Retrieves Context** - Looks up relevant information
2. ğŸ¤– **Talks to AI** - Sends questions to Azure AI
3. ğŸ“¤ **Returns Answer** - Gives you the response

```mermaid
graph TD
    Start([Agent Receives Question]) --> Retrieve[ğŸ” Step 1: Retrieve Context<br/>from Data Storage]
    Retrieve --> Build[ğŸ“ Step 2: Build Prompt<br/>Question + Context + Instructions]
    Build --> LLM[ğŸ¤– Step 3: Send to Azure AI<br/>via SDK]
    LLM --> Format[ğŸ“¦ Step 4: Format Response]
    Format --> End([Return Answer to User])
    
    style Start fill:#e1f5ff
    style End fill:#d4edda
    style LLM fill:#fff3cd
```

### Part 2: Azure AI Foundry SDK (The Connection)

**What is it?** This is the "phone line" that connects our agents to Azure's AI brain.

**How does it work?**

```mermaid
graph LR
    subgraph "Your Computer"
        Agent[ğŸ‘¨â€ğŸ³ Agent<br/>wants to ask AI]
        SDK[ğŸ“ SDK Client<br/>AzureAIFoundryClient]
    end
    
    subgraph "Microsoft Azure Cloud"
        AI[ğŸ§  GPT-4<br/>AI Model]
    end
    
    Agent -->|1. Calls| SDK
    SDK -->|2. Connects<br/>using API Key| AI
    AI -->|3. Returns<br/>Smart Answer| SDK
    SDK -->|4. Gives back| Agent
    
    style SDK fill:#fff3cd
    style AI fill:#d4edda
```

**The Code Behind It (Simplified):**

```python
# 1. CREATE THE CONNECTION
client = AsyncAzureOpenAI(
    azure_endpoint="https://your-ai.openai.azure.com",
    api_key="your-secret-key"
)

# 2. SEND A QUESTION
response = await client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful analyst"},
        {"role": "user", "content": "What is in my sales data?"}
    ]
)

# 3. GET THE ANSWER
answer = response.choices[0].message.content
```

### Part 3: Data Access Layer (The Security Guard)

**What is it?** This is like a security guard who makes sure agents can't see private data.

**Important Rule:** Agents can ONLY see:
- âœ… File names (e.g., "sales_2024.csv")
- âœ… Column names (e.g., "customer_id, revenue, date")
- âœ… Data types (e.g., "number, text, date")

**They CANNOT see:**
- âŒ Actual customer names
- âŒ Actual revenue numbers
- âŒ Any real data values

```mermaid
graph TD
    Agent[ğŸ‘¨â€ğŸ³ Agent asks:<br/>"Show me sales data"] -->|Request| Guard[ğŸ›¡ï¸ Data Access Layer<br/>Security Guard]
    
    Guard -->|Searches| RAG[ğŸ“ Azure AI Search<br/>Document Metadata]
    Guard -->|Searches| KAG[ğŸ”— Cosmos DB<br/>Knowledge Graph]
    
    RAG -->|Returns| Meta1[âœ… Metadata Only<br/>Filename: sales.csv<br/>Columns: id, revenue, date]
    KAG -->|Returns| Meta2[âœ… Metadata Only<br/>Entity: Customer<br/>Properties: name, email]
    
    Meta1 --> Guard
    Meta2 --> Guard
    Guard -->|Filtered Data| Agent
    
    style Guard fill:#f8d7da
    style Meta1 fill:#d4edda
    style Meta2 fill:#d4edda
```

---

## ğŸ¬ Step-by-Step: What Happens When You Ask a Question?

Let's say you type: **"Analyze my sales data"**

### ğŸ¯ Step 1: Orchestrator Picks the Right Agent
```mermaid
graph LR
    Q["â“ Question:<br/>Analyze my sales data"] --> O[ğŸ½ï¸ Orchestrator]
    O -->|"This needs<br/>analysis!"| A[ğŸ‘¨â€ğŸ³ Analyst Agent<br/>âœ“ Selected]
    O -.->|Not needed| R[ğŸ‘¨â€ğŸ³ Researcher<br/>âœ— Not selected]
    O -.->|Not needed| P[ğŸ‘¨â€ğŸ³ Python Coder<br/>âœ— Not selected]
    
    style A fill:#d4edda
    style R fill:#f0f0f0
    style P fill:#f0f0f0
```

### ğŸ” Step 2: Agent Looks for Context
```mermaid
graph TD
    Agent[ğŸ‘¨â€ğŸ³ Analyst Agent] -->|"Find 'sales data'"| DAL[ğŸ›¡ï¸ Data Access Layer]
    DAL -->|Searches| DB[(ğŸ—„ï¸ Storage)]
    DB --> Result["ğŸ“‹ Found:<br/>â€¢ sales_2024.csv<br/>â€¢ Columns: date, product, revenue<br/>â€¢ 1000 rows"]
    Result --> Agent
    
    style Result fill:#d4edda
```

### ğŸ¤– Step 3: Agent Builds a Prompt and Asks Azure AI
```mermaid
graph LR
    Agent[ğŸ‘¨â€ğŸ³ Agent] --> Combine["ğŸ“ Combines:<br/>1. Your question<br/>2. File metadata<br/>3. Agent instructions"]
    Combine --> Prompt["ğŸ’¬ Final Prompt:<br/>'You are an analyst.<br/>User has sales_2024.csv<br/>with date, product, revenue.<br/>Analyze this dataset.'"]
    Prompt -->|Sends via SDK| AI[ğŸ§  Azure OpenAI<br/>GPT-4]
    AI --> Answer["âœ¨ AI Response:<br/>'Your sales data contains...'"]
    
    style AI fill:#fff3cd
    style Answer fill:#d4edda
```

### ğŸ“¤ Step 4: Agent Returns the Answer to You
```mermaid
graph LR
    AI[ğŸ§  AI Response] --> Agent[ğŸ‘¨â€ğŸ³ Agent]
    Agent --> Format["ğŸ“¦ Formats as:<br/>AgentResponse object"]
    Format --> User["ğŸ‘¤ You see the answer!<br/>'Your sales data shows...'"]
    
    style User fill:#d4edda
```

---

## ğŸ› ï¸ How to Create Your Own Agent

Want to make a new agent? Follow this simple recipe:

```mermaid
graph TD
    Start([ğŸ¯ Start]) --> Step1["1ï¸âƒ£ Create folder<br/>agents/my_agent/"]
    Step1 --> Step2["2ï¸âƒ£ Create agent.py file"]
    Step2 --> Step3["3ï¸âƒ£ Write class:<br/>class MyAgent(BaseAgent)"]
    Step3 --> Step4["4ï¸âƒ£ Define system_prompt<br/>(personality & instructions)"]
    Step4 --> Step5["5ï¸âƒ£ Register in registry.py"]
    Step5 --> Done([âœ… Done!])
    
    style Start fill:#e1f5ff
    style Done fill:#d4edda
```

### Minimal Code Example:

```python
# File: agents/my_agent/agent.py
from agents.base.agent import BaseAgent

class MyAgent(BaseAgent):
    def __init__(self):
        super().__init__(
            name="My Custom Agent",
            description="Does something cool!"
        )
    
    def _get_system_prompt(self):
        return """You are a helpful assistant.
        Your job is to help users with X, Y, Z."""
    
    def _get_tools(self):
        return []  # No special tools needed
```

That's it! Your agent can now:
âœ… Access data metadata (via `self.data_layer`)  
âœ… Talk to Azure AI (via `self.llm`)  
âœ… Return smart answers (via `execute()`)

---

## ğŸ“ Summary in One Picture

```mermaid
graph TB
    subgraph "1. YOUR QUESTION"
        User([ğŸ‘¤ You ask a question])
    end
    
    subgraph "2. ROUTING"
        Orch[ğŸ½ï¸ Orchestrator picks<br/>the right agent]
    end
    
    subgraph "3. AGENT PROCESSING"
        Agent[ğŸ‘¨â€ğŸ³ Agent]
        Agent --> GetData[ğŸ” Get metadata<br/>from storage]
        Agent --> BuildPrompt[ğŸ“ Build prompt<br/>with context]
        Agent --> CallAI[ğŸ¤– Call Azure AI<br/>via SDK]
    end
    
    subgraph "4. AI MAGIC"
        AI[ğŸ§  Azure OpenAI<br/>processes request]
    end
    
    subgraph "5. YOUR ANSWER"
        Response([ğŸ“¤ You get a<br/>smart response!])
    end
    
    User --> Orch
    Orch --> Agent
    CallAI --> AI
    AI --> Response
    
    style User fill:#e1f5ff
    style AI fill:#fff3cd
    style Response fill:#d4edda
```

---

## ï¿½ Security Reminder

> [!IMPORTANT]
> **Agents are BLIND to actual data!**  
> They only see:
> - ğŸ“ Metadata (file names, column names)
> - ğŸ”— Structure (entity relationships)
> 
> They NEVER see:
> - âŒ Your actual customer names
> - âŒ Your actual revenue numbers
> - âŒ Any sensitive values

This keeps your data **safe and private**! ğŸ”’

---

## ï¿½ The Actual Code - Where is the SDK?

Let me show you the **REAL CODE** and point out exactly where the SDK is used!

### ğŸ” File 1: `backend/app/core/azure_client.py` - The SDK Wrapper

This file contains the **Azure OpenAI SDK** and wraps it for our use.

```python
# File: backend/app/core/azure_client.py
from typing import Optional, List
# â¬‡ï¸â¬‡ï¸â¬‡ï¸ THIS IS THE SDK! â¬‡ï¸â¬‡ï¸â¬‡ï¸
from openai import AsyncAzureOpenAI  # ğŸ“¦ The Azure OpenAI SDK
# â¬†ï¸â¬†ï¸â¬†ï¸ THIS IS THE SDK! â¬†ï¸â¬†ï¸â¬†ï¸

from app.core.config import settings


class AzureAIFoundryClient:
    """This class WRAPS the SDK to make it easier to use"""
    
    def __init__(self):
        # Get configuration from environment variables
        self.endpoint = settings.AZURE_OPENAI_ENDPOINT  # e.g., "https://your-ai.openai.azure.com"
        self.api_key = settings.AZURE_OPENAI_API_KEY    # Your secret key
        self.deployment = settings.AZURE_OPENAI_DEPLOYMENT  # Model name like "gpt-4"
        self.api_version = settings.AZURE_OPENAI_API_VERSION  # API version
        
        # â¬‡ï¸â¬‡ï¸â¬‡ï¸ SDK INITIALIZATION HAPPENS HERE â¬‡ï¸â¬‡ï¸â¬‡ï¸
        self._client = AsyncAzureOpenAI(
            azure_endpoint=self.endpoint,  # Where to connect
            api_key=self.api_key,          # How to authenticate
            api_version=self.api_version   # Which API version
        )
        # â¬†ï¸â¬†ï¸â¬†ï¸ NOW WE HAVE AN SDK CLIENT! â¬†ï¸â¬†ï¸â¬†ï¸
    
    async def chat_completion(self, messages: list, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """This method USES THE SDK to get AI responses"""
        
        # â¬‡ï¸â¬‡ï¸â¬‡ï¸ CALLING THE SDK'S CHAT METHOD â¬‡ï¸â¬‡ï¸â¬‡ï¸
        response = await self._client.chat.completions.create(
            model=self.deployment,      # Which AI model to use
            messages=messages,          # The conversation
            temperature=temperature,    # Creativity level (0-1)
            max_tokens=max_tokens       # Max response length
        )
        # â¬†ï¸â¬†ï¸â¬†ï¸ SDK RETURNS A RESPONSE OBJECT â¬†ï¸â¬†ï¸â¬†ï¸
        
        # Extract the text from the response
        return response.choices[0].message.content
    
    async def simple_chat(self, user_message: str, system_message: str = None) -> str:
        """Simplified interface - just ask a question!"""
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": user_message})
        
        # Calls the method above, which uses the SDK
        return await self.chat_completion(messages)
```

**Key Points:**
- ğŸ“¦ **SDK = `AsyncAzureOpenAI`** from the `openai` package
- ğŸ”§ **Line 6**: We import the SDK
- ğŸ¯ **Line 17-21**: We create an SDK client instance
- ğŸš€ **Line 28**: We call the SDK's `chat.completions.create()` method

---

### ğŸ” File 2: `agents/base/agent.py` - How Agents Use It

Now let's see how **agents actually use** the SDK wrapper:

```python
# File: agents/base/agent.py (simplified)
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any

class BaseAgent(ABC):
    """Every agent inherits from this base class"""
    
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self._llm = None  # Will hold the SDK wrapper
    
    # â¬‡ï¸â¬‡ï¸â¬‡ï¸ LAZY LOADING THE SDK WRAPPER â¬‡ï¸â¬‡ï¸â¬‡ï¸
    @property
    def llm(self):
        """Get the LLM client (SDK wrapper) when needed"""
        if self._llm is None:
            self._llm = self._initialize_llm()
        return self._llm
    # â¬†ï¸â¬†ï¸â¬†ï¸ NOW AGENT HAS ACCESS TO SDK! â¬†ï¸â¬†ï¸â¬†ï¸
    
    def _initialize_llm(self):
        """Initialize the SDK wrapper"""
        try:
            # Import the wrapper class we made above
            from app.core.azure_client import get_ai_client
            
            # â¬‡ï¸â¬‡ï¸â¬‡ï¸ GET THE SDK WRAPPER â¬‡ï¸â¬‡ï¸â¬‡ï¸
            return get_ai_client()  # Returns AzureAIFoundryClient instance
            # â¬†ï¸â¬†ï¸â¬†ï¸ WHICH CONTAINS THE SDK! â¬†ï¸â¬†ï¸â¬†ï¸
        except ImportError:
            print(f"Warning: Could not import Azure client")
            return None
    
    async def execute(self, query: str, context: Dict = None):
        """Main method - processes a user question"""
        
        # Step 1: Get relevant context from data storage
        retrieved_context = await self.retrieve_context(query)
        
        # Step 2: Build a prompt with instructions
        system_prompt = self._get_system_prompt()
        system_prompt += f"\n\n{retrieved_context['context_text']}"
        
        # Step 3: USE THE SDK (via our wrapper) to get AI response
        if self.llm:  # If SDK wrapper is available
            try:
                # â¬‡ï¸â¬‡ï¸â¬‡ï¸ THIS CALLS THE SDK! â¬‡ï¸â¬‡ï¸â¬‡ï¸
                response = await self.llm.simple_chat(
                    user_message=query,           # User's question
                    system_message=system_prompt  # Instructions + context
                )
                # â¬†ï¸â¬†ï¸â¬†ï¸ SDK RETURNS THE AI'S ANSWER! â¬†ï¸â¬†ï¸â¬†ï¸
            except Exception as llm_error:
                response = f"Error connecting to AI: {llm_error}"
        else:
            response = "AI model is not available"
        
        # Step 4: Return the response
        return response
```

**Key Points:**
- ğŸ¯ **Line 13-19**: The `llm` property gives agents access to the SDK wrapper
- ğŸ”§ **Line 28**: Gets an instance of `AzureAIFoundryClient` (which has the SDK)
- ğŸš€ **Line 49-52**: Agent calls `simple_chat()`, which internally uses the SDK

---

### ğŸ” File 3: Specific Agent Example - `analyst_agent/agent.py`

Here's how a **real agent** uses it:

```python
# File: agents/analyst_agent/agent.py
from agents.base.agent import BaseAgent

class AnalystAgent(BaseAgent):
    """This agent analyzes data"""
    
    def __init__(self):
        super().__init__(
            name="Data Analyst",
            description="Performs statistical analysis"
        )
        # Inherits self.llm from BaseAgent
        # Which contains the SDK wrapper!
    
    def _get_system_prompt(self):
        return """You are an expert data analyst.
        You can only see metadata (file names, column names).
        Provide insights based on data structure."""
    
    # When this agent's execute() method runs:
    # 1. It inherits execute() from BaseAgent
    # 2. execute() calls self.llm.simple_chat()
    # 3. simple_chat() uses the SDK (AsyncAzureOpenAI)
    # 4. SDK sends request to Azure OpenAI
    # 5. SDK gets response from Azure
    # 6. Response flows back to user
```

---

### ğŸ“Š Visual Flow - Code to SDK

```mermaid
graph TD
    A["User asks question<br/>'Analyze my data'"] --> B["AnalystAgent.execute()"]
    
    B --> C["BaseAgent.execute()<br/>(inherited)"]
    
    C --> D["self.llm.simple_chat()<br/>(get SDK wrapper)"]
    
    D --> E["AzureAIFoundryClient.simple_chat()"]
    
    E --> F["ğŸ”¥ SDK CALL ğŸ”¥<br/>AsyncAzureOpenAI.chat.completions.create()"]
    
    F --> G["â˜ï¸ Azure OpenAI Cloud<br/>(Microsoft servers)"]
    
    G --> H["ğŸ§  GPT-4 Model<br/>processes request"]
    
    H --> I["Response flows back<br/>through SDK"]
    
    I --> J["User gets answer!"]
    
    style F fill:#ff6b6b,color:#fff
    style G fill:#4dabf7,color:#fff
    style H fill:#51cf66,color:#fff
```

---

### ğŸ¯ Summary: What is the SDK and Where is it Used?

| Question | Answer |
|----------|--------|
| **What is the SDK?** | `AsyncAzureOpenAI` from the `openai` Python package |
| **Where is it imported?** | `backend/app/core/azure_client.py` (line 6) |
| **Where is it initialized?** | `AzureAIFoundryClient.__init__()` creates the SDK client |
| **Where is it used?** | In `chat_completion()` method: `self._client.chat.completions.create()` |
| **How do agents access it?** | Through `self.llm` property, which returns `AzureAIFoundryClient` |
| **When is it called?** | Every time an agent's `execute()` method needs AI response |

---

## ğŸ“š Quick Reference

| Component | Location | What It Does |
|-----------|----------|--------------|
| **SDK** | `openai.AsyncAzureOpenAI` | The actual Azure OpenAI SDK |
| **SDK Wrapper** | [azure_client.py](file:///c:/Users/sudip/Desktop/AI_Assistant/backend/app/core/azure_client.py) | Makes SDK easier to use |
| `BaseAgent` | [agent.py](file:///c:/Users/sudip/Desktop/AI_Assistant/agents/base/agent.py) | Template for all agents |
| `DataAccessLayer` | [data_access.py](file:///c:/Users/sudip/Desktop/AI_Assistant/backend/app/core/data_access.py) | Security guard for data |

---

**ğŸ‰ That's it!** Now you can see the actual code where the SDK is used!
